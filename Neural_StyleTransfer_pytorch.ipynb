{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_StyleTransfer_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66fb07d418044aa5abe567464650e55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dd57ca0f52fa4aa0a1dfb4116229ffb4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9720ce18e8ef40c699c5a0200b5c9667",
              "IPY_MODEL_c70cafe515dd49e58366ccb042527060"
            ]
          }
        },
        "dd57ca0f52fa4aa0a1dfb4116229ffb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9720ce18e8ef40c699c5a0200b5c9667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_45af9c0339aa4d589c6ae39733b9410c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd8cc1aabaf344a0a32b4e2a853dfc71"
          }
        },
        "c70cafe515dd49e58366ccb042527060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7319e0db213640d08a6b4403e49fe38f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:20&lt;00:00, 27.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ed02539f14c4791af7eee44e39e9372"
          }
        },
        "45af9c0339aa4d589c6ae39733b9410c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd8cc1aabaf344a0a32b4e2a853dfc71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7319e0db213640d08a6b4403e49fe38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ed02539f14c4791af7eee44e39e9372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsKNzmURC4OW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
        "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    \n",
        "    if max_size:\n",
        "        scale = max_size / max(image.size)\n",
        "        size = np.array(image.size) * scale\n",
        "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
        "    \n",
        "    if shape:\n",
        "        image = image.resize(shape, Image.LANCZOS)\n",
        "    \n",
        "    if transform:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    \n",
        "    return image.to(device)\n",
        "\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.select = ['0', '5', '10', '19', '28'] \n",
        "        self.vgg = models.vgg19(pretrained=True).features\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.select:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "def main(style_img_path: str,\n",
        "         content_img_path: str,\n",
        "         max_size : int,\n",
        "         num_iter: int,\n",
        "         style_weight: int,\n",
        "         content_weight: int,\n",
        "         variation_weight: int,\n",
        "         print_every: int,\n",
        "         save_every: int):\n",
        "    \n",
        "    # Image preprocessing\n",
        "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "    # We use the same normalization statistics here.\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
        "                             std=(0.229, 0.224, 0.225))])\n",
        "    \n",
        "    # Load content and style images\n",
        "    # Make the style image same size as the content image\n",
        "    content = load_image(content_img_path, transform, max_size=max_size)\n",
        "    style = load_image(style_img_path, transform, shape=[content.size(2), content.size(3)])\n",
        "    \n",
        "    # Initialize a target image with the content image\n",
        "    target = content.clone().requires_grad_(True)\n",
        "    \n",
        "    optimizer = torch.optim.Adam([target], lr= 0.01, betas=[0.5, 0.999])\n",
        "    vgg = VGGNet().to(device).eval()\n",
        "    \n",
        "    for step in range(num_iter):\n",
        "        \n",
        "        # Extract multiple(5) conv feature vectors\n",
        "        target_features = vgg(target)\n",
        "        content_features = vgg(content)\n",
        "        style_features = vgg(style)\n",
        "\n",
        "        style_loss = 0\n",
        "        content_loss = 0\n",
        "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
        "            # Compute content loss with target and content images\n",
        "            content_loss += torch.mean((f1 - f2)**2)\n",
        "\n",
        "            # Reshape convolutional feature maps\n",
        "            _, c, h, w = f1.size()\n",
        "            f1 = f1.view(c, h * w)\n",
        "            f3 = f3.view(c, h * w)\n",
        "\n",
        "            # Compute gram matrix\n",
        "            f1 = torch.mm(f1, f1.t())\n",
        "            f3 = torch.mm(f3, f3.t())\n",
        "\n",
        "            # Compute style loss with target and style images\n",
        "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
        "        \n",
        "        # Compute total loss, backprop and optimize\n",
        "        loss = content_loss * content_weight + style_weight * style_loss \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step+1) % print_every == 0:\n",
        "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
        "                   .format(step+1, num_iter, content_loss.item(), style_loss.item()))\n",
        "\n",
        "        if (step+1) % save_every == 0:\n",
        "            # Save the generated image\n",
        "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
        "            img = target.clone().squeeze()\n",
        "            img = denorm(img).clamp_(0, 1)\n",
        "            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX9oWJfcNxJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "66fb07d418044aa5abe567464650e55b",
            "dd57ca0f52fa4aa0a1dfb4116229ffb4",
            "9720ce18e8ef40c699c5a0200b5c9667",
            "c70cafe515dd49e58366ccb042527060",
            "45af9c0339aa4d589c6ae39733b9410c",
            "fd8cc1aabaf344a0a32b4e2a853dfc71",
            "7319e0db213640d08a6b4403e49fe38f",
            "4ed02539f14c4791af7eee44e39e9372"
          ]
        },
        "outputId": "6b1d4b96-c49e-4d0f-9a79-55af3a7a3bf0"
      },
      "source": [
        "style_img = '/content/styl13.jpg'\n",
        "content_img = '/content/shahrukh1.jpg'\n",
        "\n",
        "main(style_img, content_img, 512, 5000, 10e6, 10e-4, 10e3, 500, 1000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66fb07d418044aa5abe567464650e55b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step [500/5000], Content Loss: 28.4770, Style Loss: 55.9104\n",
            "Step [1000/5000], Content Loss: 29.7365, Style Loss: 29.6084\n",
            "Step [1500/5000], Content Loss: 30.1947, Style Loss: 18.0097\n",
            "Step [2000/5000], Content Loss: 30.6464, Style Loss: 12.3040\n",
            "Step [2500/5000], Content Loss: 30.8525, Style Loss: 8.9171\n",
            "Step [3000/5000], Content Loss: 30.9672, Style Loss: 6.7490\n",
            "Step [3500/5000], Content Loss: 31.1870, Style Loss: 5.2907\n",
            "Step [4000/5000], Content Loss: 30.0136, Style Loss: 22.5343\n",
            "Step [4500/5000], Content Loss: 31.3726, Style Loss: 3.6138\n",
            "Step [5000/5000], Content Loss: 31.4043, Style Loss: 3.4092\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}